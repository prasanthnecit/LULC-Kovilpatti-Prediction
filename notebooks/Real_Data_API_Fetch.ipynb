{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Satellite Data API Fetch for Kovilpatti LULC Prediction\n",
    "\n",
    "This notebook demonstrates how to fetch real satellite LULC (Land Use Land Cover) data for the Kovilpatti region using API calls. **No manual downloads required!**\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to fetch real LULC data from Microsoft Planetary Computer\n",
    "2. How to use ESA WorldCover as a fallback source\n",
    "3. How to preprocess the data for model training\n",
    "4. How to create training datasets\n",
    "5. How to verify data quality\n",
    "6. How to train models on real satellite data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's install the required packages and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pystac-client planetary-computer rasterio stackstac xarray numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.real_data.api_fetcher import KovilpattiDataFetcher, fetch_kovilpatti_data\n",
    "from src.real_data.worldcover_api import WorldCoverFetcher\n",
    "from src.real_data.preprocessor import RealDataPreprocessor, preprocess_for_model\n",
    "from src.real_data.utils import (\n",
    "    visualize_real_data, \n",
    "    print_data_summary,\n",
    "    get_kovilpatti_bbox,\n",
    "    calculate_real_transition_matrix\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch Real Data from API\n",
    "\n",
    "Now let's fetch real LULC data for Kovilpatti from Microsoft Planetary Computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Set Up Region Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kovilpatti region parameters\n",
    "REGION_NAME = \"Kovilpatti\"\n",
    "LATITUDE = 9.17  # degrees N\n",
    "LONGITUDE = 77.87  # degrees E\n",
    "RADIUS_KM = 10  # kilometers\n",
    "YEARS = [2020, 2021, 2022]  # Years to fetch\n",
    "\n",
    "# Display region info\n",
    "bbox = get_kovilpatti_bbox()\n",
    "print(f\"üìç Region: {REGION_NAME}\")\n",
    "print(f\"üåç Center: ({LATITUDE}¬∞N, {LONGITUDE}¬∞E)\")\n",
    "print(f\"üìè Radius: {RADIUS_KM} km\")\n",
    "print(f\"üì¶ Bounding Box: {bbox}\")\n",
    "print(f\"üìÖ Years: {YEARS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize Data Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fetcher\n",
    "fetcher = KovilpattiDataFetcher(\n",
    "    region_name=REGION_NAME,\n",
    "    lat=LATITUDE,\n",
    "    lon=LONGITUDE,\n",
    "    radius_km=RADIUS_KM,\n",
    "    cache_dir=\"../data/cache\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Fetcher initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fetch Data (This may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch temporal sequence\n",
    "print(\"üõ∞Ô∏è Fetching data from Planetary Computer...\\n\")\n",
    "years_data = fetcher.fetch_temporal_sequence(YEARS)\n",
    "\n",
    "if years_data:\n",
    "    print(f\"\\n‚úÖ Successfully fetched {len(years_data)} years!\")\n",
    "    for year in years_data.keys():\n",
    "        print(f\"  - {year}: {years_data[year].shape}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No data fetched. Trying fallback...\")\n",
    "    \n",
    "    # Try WorldCover as fallback\n",
    "    wc_fetcher = WorldCoverFetcher(\n",
    "        region_name=REGION_NAME,\n",
    "        lat=LATITUDE,\n",
    "        lon=LONGITUDE,\n",
    "        radius_km=RADIUS_KM,\n",
    "        cache_dir=\"../data/cache\"\n",
    "    )\n",
    "    years_data = wc_fetcher.fetch_temporal_sequence([2020, 2021])\n",
    "    \n",
    "    if years_data:\n",
    "        print(f\"\\n‚úÖ Fetched {len(years_data)} years from WorldCover!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualize Downloaded Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize each year\n",
    "for year, data in years_data.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" Year {year}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print_data_summary(data, f\"Raw LULC Data - {year}\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(data, cmap='tab20', interpolation='nearest')\n",
    "    plt.colorbar(label='LULC Class', shrink=0.7)\n",
    "    plt.title(f'Raw LULC Data - Kovilpatti {year}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Pixel X')\n",
    "    plt.ylabel('Pixel Y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Class distribution\n",
    "    unique, counts = np.unique(data, return_counts=True)\n",
    "    percentages = (counts / counts.sum()) * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(unique)), percentages, color='steelblue')\n",
    "    plt.xlabel('LULC Class')\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.title(f'Class Distribution - {year}')\n",
    "    plt.xticks(range(len(unique)), unique)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Now let's preprocess the raw data to make it suitable for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Class Remapping\n",
    "\n",
    "We need to map the raw LULC classes to our standard 7 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = RealDataPreprocessor(n_classes=7)\n",
    "\n",
    "# Remap one year as example\n",
    "year_example = list(years_data.keys())[0]\n",
    "raw_data = years_data[year_example]\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è Remapping classes for {year_example}...\")\n",
    "remapped = preprocessor.remap_classes(raw_data, source=\"io\")\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Before\n",
    "im1 = axes[0].imshow(raw_data, cmap='tab20', interpolation='nearest')\n",
    "axes[0].set_title(f'Before Remapping - {year_example}', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Pixel X')\n",
    "axes[0].set_ylabel('Pixel Y')\n",
    "plt.colorbar(im1, ax=axes[0], label='Original Class')\n",
    "\n",
    "# After\n",
    "im2 = axes[1].imshow(remapped, cmap='tab10', interpolation='nearest', vmin=0, vmax=6)\n",
    "axes[1].set_title(f'After Remapping - {year_example}', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Pixel X')\n",
    "axes[1].set_ylabel('Pixel Y')\n",
    "cbar = plt.colorbar(im2, ax=axes[1], label='Remapped Class', ticks=range(7))\n",
    "cbar.ax.set_yticklabels(['Urban', 'Forest', 'Agriculture', 'Water', 'Barren', 'Wetland', 'Grassland'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Remapping complete!\")\n",
    "print(f\"Original classes: {np.unique(raw_data)}\")\n",
    "print(f\"Remapped classes: {np.unique(remapped)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Patch Creation\n",
    "\n",
    "Large images are divided into smaller 256√ó256 patches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patches from one year\n",
    "patches = preprocessor.crop_to_patches(remapped, patch_size=256, overlap=0)\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Created {len(patches)} patches of size 256√ó256\")\n",
    "\n",
    "# Visualize some patches\n",
    "n_show = min(6, len(patches))\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_show):\n",
    "    axes[i].imshow(patches[i], cmap='tab10', interpolation='nearest', vmin=0, vmax=6)\n",
    "    axes[i].set_title(f'Patch {i+1}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Patches (256√ó256)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "print(\"\\nüîç Quality Checks:\")\n",
    "print(f\"\\n1. Data Shapes:\")\n",
    "for year, data in years_data.items():\n",
    "    print(f\"   {year}: {data.shape}\")\n",
    "\n",
    "print(f\"\\n2. Value Ranges:\")\n",
    "for year, data in years_data.items():\n",
    "    print(f\"   {year}: [{data.min()}, {data.max()}]\")\n",
    "\n",
    "print(f\"\\n3. Data Types:\")\n",
    "for year, data in years_data.items():\n",
    "    print(f\"   {year}: {data.dtype}\")\n",
    "\n",
    "print(f\"\\n4. Missing Data:\")\n",
    "for year, data in years_data.items():\n",
    "    n_nan = np.isnan(data).sum() if np.issubdtype(data.dtype, np.floating) else 0\n",
    "    print(f\"   {year}: {n_nan} NaN values\")\n",
    "\n",
    "print(\"\\n‚úÖ Quality checks complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Temporal Alignment\n",
    "\n",
    "Ensure all years have consistent spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all years have same shape\n",
    "shapes = [data.shape for data in years_data.values()]\n",
    "all_same = len(set(shapes)) == 1\n",
    "\n",
    "print(f\"\\nüìê Temporal Alignment Check:\")\n",
    "print(f\"   All years same shape: {all_same}\")\n",
    "\n",
    "if not all_same:\n",
    "    print(f\"   Shapes: {shapes}\")\n",
    "    print(f\"   ‚ö†Ô∏è Will crop to minimum dimensions during preprocessing\")\n",
    "else:\n",
    "    print(f\"   Shape: {shapes[0]}\")\n",
    "    print(f\"   ‚úÖ All years aligned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Creation\n",
    "\n",
    "Create training samples from the multi-year data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Temporal Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal samples\n",
    "print(\"\\nüì¶ Creating temporal training samples...\")\n",
    "\n",
    "splits = preprocess_for_model(\n",
    "    years_data,\n",
    "    output_dir=\"../data/Kovilpatti_LULC_Real\",\n",
    "    patch_size=256,\n",
    "    source=\"io\"  # or \"esa\" if using WorldCover\n",
    ")\n",
    "\n",
    "if splits:\n",
    "    print(\"\\n‚úÖ Dataset created successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Dataset creation failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if splits:\n",
    "    print(\"\\nüìä Dataset Statistics:\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        if split_name in splits:\n",
    "            n_samples = len(splits[split_name]['inputs'])\n",
    "            input_shape = splits[split_name]['inputs'][0].shape\n",
    "            target_shape = splits[split_name]['targets'][0].shape\n",
    "            \n",
    "            print(f\"\\n{split_name.upper()}:\")\n",
    "            print(f\"  Samples: {n_samples}\")\n",
    "            print(f\"  Input shape: {input_shape}\")\n",
    "            print(f\"  Target shape: {target_shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if splits and 'train' in splits:\n",
    "    # Show a few training samples\n",
    "    n_show = min(3, len(splits['train']['inputs']))\n",
    "    \n",
    "    for i in range(n_show):\n",
    "        input_seq = splits['train']['inputs'][i]  # Shape: (n_timesteps, H, W)\n",
    "        target = splits['train']['targets'][i]     # Shape: (H, W)\n",
    "        \n",
    "        n_timesteps = input_seq.shape[0]\n",
    "        \n",
    "        fig, axes = plt.subplots(1, n_timesteps + 1, figsize=(4*(n_timesteps+1), 4))\n",
    "        \n",
    "        # Show each input timestep\n",
    "        for t in range(n_timesteps):\n",
    "            axes[t].imshow(input_seq[t], cmap='tab10', interpolation='nearest', vmin=0, vmax=6)\n",
    "            axes[t].set_title(f'Input T-{n_timesteps-t}', fontsize=10)\n",
    "            axes[t].axis('off')\n",
    "        \n",
    "        # Show target\n",
    "        im = axes[-1].imshow(target, cmap='tab10', interpolation='nearest', vmin=0, vmax=6)\n",
    "        axes[-1].set_title('Target (Prediction)', fontsize=10, fontweight='bold')\n",
    "        axes[-1].axis('off')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[-1], ticks=range(7), shrink=0.8)\n",
    "        cbar.ax.set_yticklabels(['Urban', 'Forest', 'Agri', 'Water', 'Barren', 'Wetland', 'Grass'], fontsize=8)\n",
    "        \n",
    "        plt.suptitle(f'Training Sample {i+1}', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Data\n",
    "\n",
    "Load the saved data to verify it's in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved data\n",
    "data_dir = Path(\"../data/Kovilpatti_LULC_Real\")\n",
    "\n",
    "print(\"\\nüîç Verifying saved data...\")\n",
    "print(f\"\\nDirectory: {data_dir}\")\n",
    "\n",
    "# Check files\n",
    "required_files = [\n",
    "    'train_inputs.npy', 'train_targets.npy',\n",
    "    'val_inputs.npy', 'val_targets.npy',\n",
    "    'test_inputs.npy', 'test_targets.npy',\n",
    "    'metadata.txt'\n",
    "]\n",
    "\n",
    "print(\"\\nFiles:\")\n",
    "for filename in required_files:\n",
    "    filepath = data_dir / filename\n",
    "    exists = filepath.exists()\n",
    "    size_mb = filepath.stat().st_size / (1024**2) if exists else 0\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} {filename:20s} ({size_mb:.2f} MB)\")\n",
    "\n",
    "# Load and verify shapes\n",
    "print(\"\\nData Shapes:\")\n",
    "for split in ['train', 'val', 'test']:\n",
    "    try:\n",
    "        inputs = np.load(data_dir / f\"{split}_inputs.npy\")\n",
    "        targets = np.load(data_dir / f\"{split}_targets.npy\")\n",
    "        print(f\"  {split:5s}: inputs={inputs.shape}, targets={targets.shape}\")\n",
    "    except:\n",
    "        print(f\"  {split:5s}: ‚ùå Failed to load\")\n",
    "\n",
    "# Show metadata\n",
    "metadata_file = data_dir / \"metadata.txt\"\n",
    "if metadata_file.exists():\n",
    "    print(\"\\nMetadata:\")\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        for line in f:\n",
    "            print(f\"  {line.strip()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Transition Matrix\n",
    "\n",
    "Analyze actual land cover transitions from the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate transition matrix from real data\n",
    "if len(years_data) >= 2:\n",
    "    print(\"\\nüîÑ Calculating transition matrix from real data...\")\n",
    "    \n",
    "    # First remap all years\n",
    "    remapped_years = {}\n",
    "    for year, data in years_data.items():\n",
    "        remapped_years[year] = preprocessor.remap_classes(data, source=\"io\")\n",
    "    \n",
    "    # Calculate transitions\n",
    "    transition_matrix = calculate_real_transition_matrix(remapped_years, n_classes=7)\n",
    "    \n",
    "    # Visualize\n",
    "    class_names = ['Urban', 'Forest', 'Agriculture', 'Water', 'Barren', 'Wetland', 'Grassland']\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    im = plt.imshow(transition_matrix, cmap='YlOrRd', interpolation='nearest', vmin=0, vmax=1)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, label='Transition Probability')\n",
    "    \n",
    "    # Add labels\n",
    "    plt.xticks(range(7), class_names, rotation=45, ha='right')\n",
    "    plt.yticks(range(7), class_names)\n",
    "    plt.xlabel('To Class', fontsize=12)\n",
    "    plt.ylabel('From Class', fontsize=12)\n",
    "    plt.title('Land Cover Transition Matrix (Real Data)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            text = plt.text(j, i, f'{transition_matrix[i, j]:.2f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Transition matrix calculated and visualized!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Need at least 2 years of data for transition matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps: Train on Real Data\n",
    "\n",
    "Now that you have preprocessed real satellite data, you can train your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" üéØ NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Your data is ready at: data/Kovilpatti_LULC_Real/\")\n",
    "print(\"\\n2. Train your model using:\")\n",
    "print(\"   python scripts/run_training_real.py --data_dir data/Kovilpatti_LULC_Real/ --real_data\")\n",
    "print(\"\\n3. Or define your model here in the notebook and train interactively\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ‚ú® CONGRATULATIONS! ‚ú®\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou've successfully:\")\n",
    "print(\"  ‚úÖ Fetched real satellite data via API\")\n",
    "print(\"  ‚úÖ Preprocessed data for model training\")\n",
    "print(\"  ‚úÖ Created train/val/test splits\")\n",
    "print(\"  ‚úÖ Verified data quality\")\n",
    "print(\"  ‚úÖ Analyzed land cover transitions\")\n",
    "print(\"\\nYour LULC prediction model is ready to train on REAL satellite data!\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Fetch real satellite data** from Microsoft Planetary Computer and ESA WorldCover APIs\n",
    "2. **Preprocess the data** including class remapping and patch creation\n",
    "3. **Create temporal training samples** from multi-year data\n",
    "4. **Verify data quality** and format\n",
    "5. **Analyze land cover transitions** from real observations\n",
    "\n",
    "### Key Advantages of Using Real Data:\n",
    "\n",
    "- **Realistic patterns**: Actual land cover distributions and transitions\n",
    "- **Ground truth**: Validated satellite observations\n",
    "- **Generalization**: Models trained on real data work better on real predictions\n",
    "- **Scientific validity**: Results can be published and trusted\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/)\n",
    "- [ESA WorldCover](https://esa-worldcover.org/)\n",
    "- [Project Documentation](../docs/REAL_DATA_GUIDE.md)\n",
    "\n",
    "Happy modeling! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
